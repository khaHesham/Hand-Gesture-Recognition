{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initial code for our Hand Gesture Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML models imports\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.feature import hog, local_binary_pattern\n",
    "\n",
    "from skimage.filters import try_all_threshold\n",
    "\n",
    "# destribution models imports\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Data manipulation imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm as tqdm\n",
    "\n",
    "# visualisation models imports\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# image processing imports\n",
    "import skimage.io as io\n",
    "import cv2\n",
    "from skimage.transform import resize\n",
    "\n",
    "# dealing with files\n",
    "import os\n",
    "\n",
    "# visual dataset (to test randomized gridsearch not needed for now)\n",
    "from sklearn.datasets import make_hastie_10_2  # to test our models\n",
    "\n",
    "# from utils import prepareData, LoadData, FeatureExtraction, preprocess\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment(image):\n",
    "    blured_image = cv2.GaussianBlur(image, (7, 7), 0)\n",
    "    \n",
    "    # Extract the Cr channel\n",
    "    ycbcr_image = cv2.cvtColor(blured_image, cv2.COLOR_BGR2YCrCb)\n",
    "    cr_channel = ycbcr_image[:,:,1]\n",
    "\n",
    "    # Apply thresholding to obtain a binary image\n",
    "    _, binary_img = cv2.threshold(cr_channel,0,255,cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "\n",
    "    # Define the structuring element for the closing operation\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (10, 10))\n",
    "    # Perform the closing operation\n",
    "    closed_img = cv2.dilate(binary_img, kernel, iterations=1)\n",
    "\n",
    "    # Find the contours in the binary image\n",
    "    contours, hierarchy = cv2.findContours(closed_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    segmented_image = closed_img.copy()\n",
    "    cv2.drawContours(segmented_image, contours, -1, 255, -1)\n",
    "    \n",
    "    # preprocessed = cv2.bitwise_and(image, image, mask=segmented_image)\n",
    "    # preprocessed = cv2.cvtColor(preprocessed, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    return segmented_image, contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import find_peaks\n",
    "\n",
    "def count_peaks(image, contours):\n",
    "    M = cv2.moments(image)\n",
    "    \n",
    "    cX = int(M[\"m10\"] / M[\"m00\"])\n",
    "    cY = int(M[\"m01\"] / M[\"m00\"])\n",
    "    centroid = np.array([cX, cY])\n",
    "\n",
    "    # Find the contour with the maximum area\n",
    "    max_contour = max(contours, key=cv2.contourArea)\n",
    "    max_contour = np.squeeze(max_contour)\n",
    "    # Find peaks in the x-coordinate of contour points\n",
    "    x = max_contour[:, 0]\n",
    "    \n",
    "    m, n = image.shape\n",
    "    if(centroid[0] < n/2): # the hand is pointing to right\n",
    "        peak_indices, _ = find_peaks(x, distance=200)\n",
    "    else:  # the hand is pointing to left\n",
    "        peak_indices, _ = find_peaks(-x, distance=200)\n",
    "    \n",
    "    peaks = max_contour[peak_indices]\n",
    "    \n",
    "    if len(peak_indices) == 0: return 0\n",
    "\n",
    "    distance = np.linalg.norm(peaks - centroid, axis=1)\n",
    "    max_peak_distance = np.max(distance)\n",
    "\n",
    "    significant_peaks = peaks[distance >= 0.75*max_peak_distance]\n",
    "\n",
    "    # Draw circles around peak points on the original image\n",
    "    # for peak in peaks:\n",
    "    #     cv2.circle(image, peak, 50, (0, 255, 0), -1)\n",
    "    \n",
    "    return(len(significant_peaks))\n",
    "\n",
    "def ratio(contours):\n",
    "    # Find the contour with the maximum area\n",
    "    max_contour = max(contours, key=cv2.contourArea)\n",
    "    max_contour = np.squeeze(max_contour)\n",
    "\n",
    "    area = cv2.contourArea(max_contour)\n",
    "    x, y, w, h = cv2.boundingRect(max_contour)\n",
    "    area_bounding_rectangle = w*h\n",
    "    ratio = area/area_bounding_rectangle\n",
    "    return ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_all(img, sigma=7, sec=25, block_size=11, threshold=cv2.ADAPTIVE_THRESH_GAUSSIAN_C):\n",
    "    blured_image = cv2.GaussianBlur(img, (sigma, sigma), 0)\n",
    "    \n",
    "    gray = cv2.cvtColor(blured_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # gray = cv2.cvtColor(blured_image, cv2.COLOR_BGR2GRAY)\n",
    "    binary_img = cv2.adaptiveThreshold(gray, 255, threshold, cv2.THRESH_BINARY_INV, block_size, 2)\n",
    "    \n",
    "    \n",
    "    # Define the structuring element for the closing operation\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (sec, sec))\n",
    "    # Perform the closing operation\n",
    "    closed_img = cv2.morphologyEx(binary_img, cv2.MORPH_CLOSE, kernel)\n",
    "    \n",
    "    contours, hierarchy = cv2.findContours(closed_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    segmented_image = closed_img.copy()\n",
    "    cv2.drawContours(segmented_image, contours, -1, 255, -1)\n",
    "    \n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (sec, sec))\n",
    "    segmented_image = cv2.morphologyEx(segmented_image, cv2.MORPH_CLOSE, kernel)\n",
    "    \n",
    "    return segmented_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FeatureExtraction(image):\n",
    "        \n",
    "    # Extract the hog features\n",
    "    # block_norm uses L2 norm with hysterisis for reducing effect of illuminacity\n",
    "    # transform_sqrt for applying gamma correction\n",
    "    preprocessed_image, contours = segment(image)\n",
    "    \n",
    "    resized_image = resize(preprocessed_image,(64,128))\n",
    "\n",
    "    hog_features = hog(resized_image, block_norm='L2-Hys', feature_vector=True, transform_sqrt=True, pixels_per_cell=(12, 12), cells_per_block=(2, 2))\n",
    "    contour_to_ROI = ratio(contours)\n",
    "    peak_num = count_peaks(preprocessed_image, contours)\n",
    "    \n",
    "    features = np.append(hog_features, (contour_to_ROI, peak_num))\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadData():\n",
    "    Features=[]\n",
    "    labels=[]\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    \n",
    "    for gender in [\"men\",\"Women\"]:\n",
    "        datadir = r\"Dataset\\{}\".format(gender)\n",
    "        # loop over gender\n",
    "        for hand in os.listdir(datadir): \n",
    "            # loop over each class [0,1,2,3,4,5]\n",
    "            for img in os.listdir(datadir+ \"/\" +str(hand)):\n",
    "                # ignoring anything except images\n",
    "                if((img.split('.')[-1]).lower() not in ['jpg','png','jpeg']):\n",
    "                    continue\n",
    "\n",
    "                # loading our images\n",
    "                img_array=io.imread(datadir + \"/\" + str(hand) + \"/\" + img )  # approx 2500 * 4000\n",
    "\n",
    "                # append extracted features to Featurees list   \n",
    "                Feature = FeatureExtraction(img_array)        \n",
    "                Features.append(Feature) \n",
    "\n",
    "                # append class of image.\n",
    "                labels.append(hand)  \n",
    "\n",
    "                print(f'image Number: {i}')\n",
    "                i+=1\n",
    "                # # print(f'saving block : {i//100}')\n",
    "                # # np.save(f'Features/{i}.npy', Feature)\n",
    "                # # Features = []\n",
    "                # # labels = []\n",
    "                # with open(\"Features.csv\", 'a') as csvfile:\n",
    "                #     csvwriter = csv.writer(csvfile)\n",
    "                #     csvwriter.writerow(Feature)\n",
    "\n",
    "    return np.asarray(Features),np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Features, labels = LoadData() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* # Selecting the best model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Define hyperparameter grids for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distributions = {\n",
    "    'RandomForestClassifier': {\n",
    "        'n_estimators': randint(50, 500),\n",
    "        'max_depth': randint(2, 20),\n",
    "        'min_samples_split': randint(2, 10),\n",
    "        'min_samples_leaf': randint(1, 5),\n",
    "        'max_features': ['sqrt', 'log2']\n",
    "    },\n",
    "    'GradientBoostingClassifier': {\n",
    "        'learning_rate': uniform(0.01, 0.2),\n",
    "        'n_estimators': randint(50, 500),\n",
    "        'max_depth': randint(2, 20),\n",
    "        'min_samples_split': randint(2, 10),\n",
    "        'min_samples_leaf': randint(1, 5),\n",
    "        'max_features': ['sqrt', 'log2']\n",
    "    },\n",
    "    'SVC': {\n",
    "        'C': uniform(0.01, 10),\n",
    "        'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "        'degree': randint(2, 5),\n",
    "        'gamma': ['scale', 'auto'] + list(np.arange(0.1, 1, 0.1))\n",
    "    },\n",
    "    'LogisticRegression': {\n",
    "        'C': uniform(0.01, 10),\n",
    "        'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "        'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "        'max_iter': randint(50, 500)\n",
    "    },\n",
    "    'DecisionTreeClassifier': {\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "        'splitter': ['best', 'random'],\n",
    "        'max_depth': randint(2, 20),\n",
    "        'min_samples_split': randint(2, 10),\n",
    "        'min_samples_leaf': randint(1, 5),\n",
    "        'max_features': ['sqrt', 'log2']\n",
    "    },\n",
    "    'KNeighborsClassifier': {\n",
    "        'n_neighbors': randint(3, 30),\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'algorithm': ['ball_tree', 'kd_tree', 'brute'],\n",
    "        'leaf_size': randint(10, 100)\n",
    "    },\n",
    "    'GaussianNB': {\n",
    "        'var_smoothing': uniform(1e-09, 1e-07)\n",
    "    },\n",
    "    'MLPClassifier': {\n",
    "        'hidden_layer_sizes': [(50, 50), (100,), (100, 50)],\n",
    "        'activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "        'solver': ['lbfgs', 'sgd', 'adam'],\n",
    "        'alpha': uniform(0.0001, 0.01),\n",
    "        'max_iter': randint(100, 1000)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Create a list of models to train (as example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    RandomForestClassifier(),\n",
    "    GradientBoostingClassifier(),\n",
    "    SVC(),\n",
    "    KNeighborsClassifier(),\n",
    "    GaussianNB()\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Loop over the models and fit  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load our dummy data to test the randomizedSearche function\n",
    "x,y = Features,labels\n",
    "df = pd.DataFrame(x)\n",
    "df['Y'] = y\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2) # this function shuffles the data points, and splits the data into\n",
    "                                                  # 80% training set and 20% test set (indicated by test_size=0.2)\n",
    "X_train, Y_train = train.iloc[:, :-1], train.iloc[:, -1]\n",
    "X_test, Y_test = test.iloc[:, :-1], test.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, model in enumerate(models):\n",
    "    print(f'Training Model {i+1}/{len(models)}: {str(model)[:-2]}')\n",
    "    # Define randomized grid search\n",
    "    random_search = RandomizedSearchCV(model, param_distributions[str(model)[:-2]], n_iter=10,cv=5, n_jobs=-1) # n_jobs means number of jobs to run in parallel. None means 1,\n",
    "                                                                                                                # -1 means using all processors ðŸ˜ˆ.\n",
    "    # Fit the randomized grid search to the data\n",
    "    random_search.fit(X_train, Y_train)\n",
    "    print(f'Best score: {random_search.best_score_:.3f}')\n",
    "    print(f'Best parameters: {random_search.best_params_}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
