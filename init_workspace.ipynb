{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initial code for our Hand Gesture Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML models imports\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.feature import hog, local_binary_pattern\n",
    "\n",
    "from skimage.filters import try_all_threshold\n",
    "\n",
    "# destribution models imports\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Data manipulation imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm as tqdm\n",
    "import joblib\n",
    "\n",
    "# visualisation models imports\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# image processing imports\n",
    "import skimage.io as io\n",
    "import cv2\n",
    "from skimage.transform import resize\n",
    "\n",
    "# dealing with files\n",
    "import os\n",
    "\n",
    "# visual dataset (to test randomized gridsearch not needed for now)\n",
    "from sklearn.datasets import make_hastie_10_2  # to test our models\n",
    "\n",
    "# from utils import prepareData, LoadData, FeatureExtraction, preprocess\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment(image):\n",
    "    blured_image = cv2.GaussianBlur(image, (7, 7), 0)\n",
    "    \n",
    "    # Extract the Cr channel\n",
    "    ycbcr_image = cv2.cvtColor(blured_image, cv2.COLOR_BGR2YCrCb)\n",
    "    cr_channel = ycbcr_image[:,:,1]\n",
    "\n",
    "    # Apply thresholding to obtain a binary image\n",
    "    _, binary_img = cv2.threshold(cr_channel,0,255,cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "\n",
    "    # Define the structuring element for the closing operation\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (10, 10))\n",
    "    # Perform the closing operation\n",
    "    closed_img = cv2.dilate(binary_img, kernel, iterations=1)\n",
    "\n",
    "    # Find the contours in the binary image\n",
    "    contours, hierarchy = cv2.findContours(closed_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    segmented_image = closed_img.copy()\n",
    "    cv2.drawContours(segmented_image, contours, -1, 255, -1)\n",
    "    \n",
    "    # preprocessed = cv2.bitwise_and(image, image, mask=segmented_image)\n",
    "    # preprocessed = cv2.cvtColor(preprocessed, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    return segmented_image, contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import find_peaks\n",
    "\n",
    "def count_peaks(image, contours):\n",
    "    M = cv2.moments(image)\n",
    "    \n",
    "    cX = int(M[\"m10\"] / M[\"m00\"])\n",
    "    cY = int(M[\"m01\"] / M[\"m00\"])\n",
    "    centroid = np.array([cX, cY])\n",
    "\n",
    "    # Find the contour with the maximum area\n",
    "    max_contour = max(contours, key=cv2.contourArea)\n",
    "    max_contour = np.squeeze(max_contour)\n",
    "    # Find peaks in the x-coordinate of contour points\n",
    "    x = max_contour[:, 0]\n",
    "    \n",
    "    m, n = image.shape\n",
    "    if(centroid[0] < n/2): # the hand is pointing to right\n",
    "        peak_indices, _ = find_peaks(x, distance=200)\n",
    "    else:  # the hand is pointing to left\n",
    "        peak_indices, _ = find_peaks(-x, distance=200)\n",
    "    \n",
    "    peaks = max_contour[peak_indices]\n",
    "    \n",
    "    if len(peak_indices) == 0: return 0\n",
    "\n",
    "    distance = np.linalg.norm(peaks - centroid, axis=1)\n",
    "    max_peak_distance = np.max(distance)\n",
    "\n",
    "    significant_peaks = peaks[distance >= 0.75*max_peak_distance]\n",
    "\n",
    "    # Draw circles around peak points on the original image\n",
    "    # for peak in peaks:\n",
    "    #     cv2.circle(image, peak, 50, (0, 255, 0), -1)\n",
    "    \n",
    "    return(len(significant_peaks))\n",
    "\n",
    "def ratio(contours):\n",
    "    # Find the contour with the maximum area\n",
    "    max_contour = max(contours, key=cv2.contourArea)\n",
    "    max_contour = np.squeeze(max_contour)\n",
    "\n",
    "    area = cv2.contourArea(max_contour)\n",
    "    x, y, w, h = cv2.boundingRect(max_contour)\n",
    "    area_bounding_rectangle = w*h\n",
    "    ratio = area/area_bounding_rectangle\n",
    "    return ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FeatureExtraction(image):\n",
    "        \n",
    "    # Extract the hog features\n",
    "    # block_norm uses L2 norm with hysterisis for reducing effect of illuminacity\n",
    "    # transform_sqrt for applying gamma correction\n",
    "    preprocessed_image, contours = segment(image)\n",
    "    \n",
    "    resized_image = resize(preprocessed_image,(64,128))\n",
    "\n",
    "    # hog_features = hog(resized_image, block_norm='L2-Hys', feature_vector=True, transform_sqrt=True, pixels_per_cell=(12, 12), cells_per_block=(2, 2))\n",
    "    contour_to_ROI = ratio(contours)\n",
    "    peak_num = count_peaks(preprocessed_image, contours)\n",
    "    \n",
    "    # features = np.append(hog_features, (contour_to_ROI, peak_num))\n",
    "    features = np.array([contour_to_ROI, peak_num])\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testOurModel(path):\n",
    "    '''\n",
    "       this is a utility function used to load the test data\n",
    "    '''\n",
    "\n",
    "    Features = []\n",
    "    datadir = path\n",
    "    # loop over gender\n",
    "    counter=1\n",
    "    for img in sorted(os.listdir(datadir)):\n",
    "        # ignoring anything except images\n",
    "        if((img.split('.')[-1]).lower() not in ['jpg', 'png', 'jpeg']):\n",
    "            continue\n",
    "        if counter>54:\n",
    "            ourPath= datadir + \"/\" + '(' + str(counter)+\").jpeg\"\n",
    "        else:\n",
    "            ourPath= datadir + \"/\" + '(' + str(counter)+\").jpg\"   \n",
    "        print(ourPath)\n",
    "        # loading our images\n",
    "        img_array = io.imread(ourPath)\n",
    "        counter=counter+1\n",
    "        # append extracted features to Featurees list\n",
    "        Features.append(FeatureExtraction(img_array))\n",
    "\n",
    "    # here i have features need to pass them to my model\n",
    "\n",
    "    svm = joblib.load('kiro_model.pkl')\n",
    "\n",
    "    y_pred = svm.predict(pd.DataFrame(Features))\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def readFinalResults(resultFilePath):\n",
    "    ''' \n",
    "        this is a utility function used to read the \n",
    "        file which contains the actual results for \n",
    "        the test data. \n",
    "        input:\n",
    "            resultFilePath: the path of the file which contains the actual results for the test data.\n",
    "        output:\n",
    "            results: the actual results for the test data.\n",
    "    '''\n",
    "    f = open(resultFilePath, 'r')\n",
    "    results = f.read()\n",
    "    f.close()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadData():\n",
    "    Features=[]\n",
    "    labels=[]\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    \n",
    "    for gender in [\"men\",\"Women\"]:\n",
    "        datadir = r\"Sample\\{}\".format(gender)\n",
    "        # loop over gender\n",
    "        for hand in os.listdir(datadir): \n",
    "            # loop over each class [0,1,2,3,4,5]\n",
    "            for img in os.listdir(datadir+ \"/\" +str(hand)):\n",
    "                # ignoring anything except images\n",
    "                if((img.split('.')[-1]).lower() not in ['jpg','png','jpeg']):\n",
    "                    continue\n",
    "\n",
    "                # loading our images\n",
    "                img_array=io.imread(datadir + \"/\" + str(hand) + \"/\" + img )  # approx 2500 * 4000\n",
    "\n",
    "                # append extracted features to Featurees list   \n",
    "                Feature = FeatureExtraction(img_array)        \n",
    "                Features.append(Feature) \n",
    "\n",
    "                # append class of image.\n",
    "                labels.append(hand)  \n",
    "\n",
    "                print(f'image Number: {i}')\n",
    "                i+=1\n",
    "                # # print(f'saving block : {i//100}')\n",
    "                # # np.save(f'Features/{i}.npy', Feature)\n",
    "                # # Features = []\n",
    "                # # labels = []\n",
    "                # with open(\"Features.csv\", 'a') as csvfile:\n",
    "                #     csvwriter = csv.writer(csvfile)\n",
    "                #     csvwriter.writerow(Feature)\n",
    "\n",
    "    return np.asarray(Features),np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image Number: 0\n",
      "image Number: 1\n",
      "image Number: 2\n",
      "image Number: 3\n",
      "image Number: 4\n",
      "image Number: 5\n",
      "image Number: 6\n",
      "image Number: 7\n",
      "image Number: 8\n",
      "image Number: 9\n",
      "image Number: 10\n",
      "image Number: 11\n",
      "image Number: 12\n",
      "image Number: 13\n",
      "image Number: 14\n",
      "image Number: 15\n",
      "image Number: 16\n",
      "image Number: 17\n",
      "image Number: 18\n",
      "image Number: 19\n",
      "image Number: 20\n",
      "image Number: 21\n",
      "image Number: 22\n",
      "image Number: 23\n",
      "image Number: 24\n",
      "image Number: 25\n",
      "image Number: 26\n",
      "image Number: 27\n",
      "image Number: 28\n",
      "image Number: 29\n",
      "image Number: 30\n",
      "image Number: 31\n",
      "image Number: 32\n",
      "image Number: 33\n",
      "image Number: 34\n",
      "image Number: 35\n",
      "image Number: 36\n",
      "image Number: 37\n",
      "image Number: 38\n",
      "image Number: 39\n",
      "image Number: 40\n",
      "image Number: 41\n",
      "image Number: 42\n",
      "image Number: 43\n",
      "image Number: 44\n",
      "image Number: 45\n",
      "image Number: 46\n",
      "image Number: 47\n",
      "image Number: 48\n",
      "image Number: 49\n",
      "image Number: 50\n",
      "image Number: 51\n",
      "image Number: 52\n",
      "image Number: 53\n",
      "image Number: 54\n",
      "image Number: 55\n",
      "image Number: 56\n",
      "image Number: 57\n",
      "image Number: 58\n",
      "image Number: 59\n",
      "image Number: 60\n",
      "image Number: 61\n",
      "image Number: 62\n",
      "image Number: 63\n",
      "image Number: 64\n",
      "image Number: 65\n",
      "image Number: 66\n",
      "image Number: 67\n",
      "image Number: 68\n",
      "image Number: 69\n",
      "image Number: 70\n",
      "image Number: 71\n",
      "image Number: 72\n",
      "image Number: 73\n",
      "image Number: 74\n",
      "image Number: 75\n",
      "image Number: 76\n",
      "image Number: 77\n",
      "image Number: 78\n",
      "image Number: 79\n",
      "image Number: 80\n",
      "image Number: 81\n",
      "image Number: 82\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[95], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m Features, labels \u001b[39m=\u001b[39m LoadData() \n",
      "Cell \u001b[1;32mIn[91], line 22\u001b[0m, in \u001b[0;36mLoadData\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m img_array\u001b[39m=\u001b[39mio\u001b[39m.\u001b[39mimread(datadir \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(hand) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m img )  \u001b[39m# approx 2500 * 4000\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[39m# append extracted features to Featurees list   \u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m Feature \u001b[39m=\u001b[39m FeatureExtraction(img_array)        \n\u001b[0;32m     23\u001b[0m Features\u001b[39m.\u001b[39mappend(Feature) \n\u001b[0;32m     25\u001b[0m \u001b[39m# append class of image.\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[63], line 8\u001b[0m, in \u001b[0;36mFeatureExtraction\u001b[1;34m(image)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mFeatureExtraction\u001b[39m(image):\n\u001b[0;32m      2\u001b[0m         \n\u001b[0;32m      3\u001b[0m     \u001b[39m# Extract the hog features\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     \u001b[39m# block_norm uses L2 norm with hysterisis for reducing effect of illuminacity\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     \u001b[39m# transform_sqrt for applying gamma correction\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     preprocessed_image, contours \u001b[39m=\u001b[39m segment(image)\n\u001b[1;32m----> 8\u001b[0m     resized_image \u001b[39m=\u001b[39m resize(preprocessed_image,(\u001b[39m64\u001b[39;49m,\u001b[39m128\u001b[39;49m))\n\u001b[0;32m     10\u001b[0m     \u001b[39m# hog_features = hog(resized_image, block_norm='L2-Hys', feature_vector=True, transform_sqrt=True, pixels_per_cell=(12, 12), cells_per_block=(2, 2))\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     contour_to_ROI \u001b[39m=\u001b[39m ratio(contours)\n",
      "File \u001b[1;32mc:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\skimage\\transform\\_warps.py:181\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(image, output_shape, order, mode, cval, clip, preserve_range, anti_aliasing, anti_aliasing_sigma)\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[39melif\u001b[39;00m np\u001b[39m.\u001b[39many((anti_aliasing_sigma \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m) \u001b[39m&\u001b[39m (factors \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)):\n\u001b[0;32m    179\u001b[0m             warn(\u001b[39m\"\u001b[39m\u001b[39mAnti-aliasing standard deviation greater than zero but \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    180\u001b[0m                  \u001b[39m\"\u001b[39m\u001b[39mnot down-sampling along all axes\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 181\u001b[0m     image \u001b[39m=\u001b[39m ndi\u001b[39m.\u001b[39;49mgaussian_filter(image, anti_aliasing_sigma,\n\u001b[0;32m    182\u001b[0m                                 cval\u001b[39m=\u001b[39;49mcval, mode\u001b[39m=\u001b[39;49mndi_mode)\n\u001b[0;32m    184\u001b[0m \u001b[39mif\u001b[39;00m NumpyVersion(scipy\u001b[39m.\u001b[39m__version__) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m1.6.0\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    185\u001b[0m     \u001b[39m# The grid_mode kwarg was introduced in SciPy 1.6.0\u001b[39;00m\n\u001b[0;32m    186\u001b[0m     zoom_factors \u001b[39m=\u001b[39m [\u001b[39m1\u001b[39m \u001b[39m/\u001b[39m f \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m factors]\n",
      "File \u001b[1;32mc:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\ndimage\\_filters.py:368\u001b[0m, in \u001b[0;36mgaussian_filter\u001b[1;34m(input, sigma, order, output, mode, cval, truncate, radius)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(axes) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    367\u001b[0m     \u001b[39mfor\u001b[39;00m axis, sigma, order, mode, radius \u001b[39min\u001b[39;00m axes:\n\u001b[1;32m--> 368\u001b[0m         gaussian_filter1d(\u001b[39minput\u001b[39;49m, sigma, axis, order, output,\n\u001b[0;32m    369\u001b[0m                           mode, cval, truncate, radius\u001b[39m=\u001b[39;49mradius)\n\u001b[0;32m    370\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m output\n\u001b[0;32m    371\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\ndimage\\_filters.py:276\u001b[0m, in \u001b[0;36mgaussian_filter1d\u001b[1;34m(input, sigma, axis, order, output, mode, cval, truncate, radius)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[39m# Since we are calling correlate, not convolve, revert the kernel\u001b[39;00m\n\u001b[0;32m    275\u001b[0m weights \u001b[39m=\u001b[39m _gaussian_kernel1d(sigma, order, lw)[::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m--> 276\u001b[0m \u001b[39mreturn\u001b[39;00m correlate1d(\u001b[39minput\u001b[39;49m, weights, axis, output, mode, cval, \u001b[39m0\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\ndimage\\_filters.py:134\u001b[0m, in \u001b[0;36mcorrelate1d\u001b[1;34m(input, weights, axis, output, mode, cval, origin)\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mInvalid origin; origin must satisfy \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    131\u001b[0m                      \u001b[39m'\u001b[39m\u001b[39m-(len(weights) // 2) <= origin <= \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    132\u001b[0m                      \u001b[39m'\u001b[39m\u001b[39m(len(weights)-1) // 2\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    133\u001b[0m mode \u001b[39m=\u001b[39m _ni_support\u001b[39m.\u001b[39m_extend_mode_to_code(mode)\n\u001b[1;32m--> 134\u001b[0m _nd_image\u001b[39m.\u001b[39;49mcorrelate1d(\u001b[39minput\u001b[39;49m, weights, axis, output, mode, cval,\n\u001b[0;32m    135\u001b[0m                       origin)\n\u001b[0;32m    136\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Features, labels = LoadData() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load our dummy data to test the randomizedSearche function\n",
    "x,y = Features,labels\n",
    "df = pd.DataFrame(x)\n",
    "df['Y'] = y\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2) # this function shuffles the data points, and splits the data into\n",
    "                                                  # 80% training set and 20% test set (indicated by test_size=0.2)\n",
    "X_train, Y_train = train.iloc[:, :-1], train.iloc[:, -1]\n",
    "X_test, Y_test = test.iloc[:, :-1], test.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=8.948624388292055, coef0=1, degree=4, gamma=5, kernel=&#x27;poly&#x27;,\n",
       "    max_iter=100, probability=True, tol=1e-05)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=8.948624388292055, coef0=1, degree=4, gamma=5, kernel=&#x27;poly&#x27;,\n",
       "    max_iter=100, probability=True, tol=1e-05)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(C=8.948624388292055, coef0=1, degree=4, gamma=5, kernel='poly',\n",
       "    max_iter=100, probability=True, tol=1e-05)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm = SVC(C= 8.948624388292055, coef0 = 1, degree= 4, gamma= 5, kernel= 'poly', max_iter= 1000, probability= True, shrinking= True ,tol= 1e-05) \n",
    "svm.fit(X_train, Y_train)\n",
    "joblib.dump(svm, 'kiro_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testData/(1).jpg\n",
      "testData/(2).jpg\n",
      "testData/(3).jpg\n",
      "testData/(4).jpg\n",
      "testData/(5).jpg\n",
      "testData/(6).jpg\n",
      "testData/(7).jpg\n",
      "testData/(8).jpg\n",
      "testData/(9).jpg\n",
      "testData/(10).jpg\n",
      "testData/(11).jpg\n",
      "testData/(12).jpg\n",
      "testData/(13).jpg\n",
      "testData/(14).jpg\n",
      "testData/(15).jpg\n",
      "testData/(16).jpg\n",
      "testData/(17).jpg\n",
      "testData/(18).jpg\n",
      "testData/(19).jpg\n",
      "testData/(20).jpg\n",
      "testData/(21).jpg\n",
      "testData/(22).jpg\n",
      "testData/(23).jpg\n",
      "testData/(24).jpg\n",
      "testData/(25).jpg\n",
      "testData/(26).jpg\n",
      "testData/(27).jpg\n",
      "testData/(28).jpg\n",
      "testData/(29).jpg\n",
      "testData/(30).jpg\n",
      "testData/(31).jpg\n",
      "testData/(32).jpg\n",
      "testData/(33).jpg\n",
      "testData/(34).jpg\n",
      "testData/(35).jpg\n",
      "testData/(36).jpg\n",
      "testData/(37).jpg\n",
      "testData/(38).jpg\n",
      "testData/(39).jpg\n",
      "testData/(40).jpg\n",
      "testData/(41).jpg\n",
      "testData/(42).jpg\n",
      "testData/(43).jpg\n",
      "testData/(44).jpg\n",
      "testData/(45).jpg\n",
      "testData/(46).jpg\n",
      "testData/(47).jpg\n",
      "testData/(48).jpg\n",
      "testData/(49).jpg\n",
      "testData/(50).jpg\n",
      "testData/(51).jpg\n",
      "testData/(52).jpg\n",
      "testData/(53).jpg\n",
      "testData/(54).jpg\n",
      "testData/(55).jpeg\n",
      "testData/(56).jpeg\n",
      "testData/(57).jpeg\n",
      "testData/(58).jpeg\n",
      "testData/(59).jpeg\n",
      "testData/(60).jpeg\n",
      "testData/(61).jpeg\n",
      "testData/(62).jpeg\n",
      "testData/(63).jpeg\n",
      "testData/(64).jpeg\n",
      "testData/(65).jpeg\n",
      "testData/(66).jpeg\n",
      "testData/(67).jpeg\n",
      "testData/(68).jpeg\n",
      "testData/(69).jpeg\n",
      "testData/(70).jpeg\n",
      "testData/(71).jpeg\n",
      "testData/(72).jpeg\n",
      "testData/(73).jpeg\n",
      "testData/(74).jpeg\n",
      "testData/(75).jpeg\n",
      "testData/(76).jpeg\n",
      "testData/(77).jpeg\n",
      "testData/(78).jpeg\n",
      "testData/(79).jpeg\n",
      "testData/(80).jpeg\n",
      "testData/(81).jpeg\n",
      "testData/(82).jpeg\n",
      "testData/(83).jpeg\n",
      "testData/(84).jpeg\n",
      "testData/(85).jpeg\n",
      "testData/(86).jpeg\n",
      "testData/(87).jpeg\n",
      "testData/(88).jpeg\n",
      "testData/(89).jpeg\n",
      "testData/(90).jpeg\n",
      "testData/(91).jpeg\n",
      "testData/(92).jpeg\n",
      "testData/(93).jpeg\n",
      "testData/(94).jpeg\n",
      "testData/(95).jpeg\n",
      "testData/(96).jpeg\n",
      "testData/(97).jpeg\n",
      "testData/(98).jpeg\n",
      "testData/(99).jpeg\n",
      "testData/(100).jpeg\n",
      "testData/(101).jpeg\n",
      "testData/(102).jpeg\n",
      "testData/(103).jpeg\n",
      "testData/(104).jpeg\n",
      "testData/(105).jpeg\n",
      "testData/(106).jpeg\n",
      "testData/(107).jpeg\n",
      "testData/(108).jpeg\n",
      "testData/(109).jpeg\n",
      "testData/(110).jpeg\n",
      "testData/(111).jpeg\n",
      "testData/(112).jpeg\n",
      "testData/(113).jpeg\n",
      "testData/(114).jpeg\n",
      "testData/(115).jpeg\n"
     ]
    }
   ],
   "source": [
    "labels = readFinalResults('results_set_2.txt')\n",
    "y_pred = testOurModel('testData')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6534090909090908\n"
     ]
    }
   ],
   "source": [
    "print(1-(np.sum(labels[54:] != y_pred[54:]))/len(labels[54:]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* # Selecting the best model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Define hyperparameter grids for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distributions = {\n",
    "    'RandomForestClassifier': {\n",
    "        'n_estimators': randint(50, 500),\n",
    "        'max_depth': randint(2, 20),\n",
    "        'min_samples_split': randint(2, 10),\n",
    "        'min_samples_leaf': randint(1, 5),\n",
    "        'max_features': ['sqrt', 'log2']\n",
    "    },\n",
    "    'GradientBoostingClassifier': {\n",
    "        'learning_rate': uniform(0.01, 0.2),\n",
    "        'n_estimators': randint(50, 500),\n",
    "        'max_depth': randint(2, 20),\n",
    "        'min_samples_split': randint(2, 10),\n",
    "        'min_samples_leaf': randint(1, 5),\n",
    "        'max_features': ['sqrt', 'log2']\n",
    "    },\n",
    "    'SVC': {\n",
    "        'C': uniform(0.01, 10),\n",
    "        'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "        'degree': randint(2, 5),\n",
    "        'gamma': ['scale', 'auto'] + list(np.arange(0.1, 1, 0.1))\n",
    "    },\n",
    "    'LogisticRegression': {\n",
    "        'C': uniform(0.01, 10),\n",
    "        'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "        'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "        'max_iter': randint(50, 500)\n",
    "    },\n",
    "    'DecisionTreeClassifier': {\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "        'splitter': ['best', 'random'],\n",
    "        'max_depth': randint(2, 20),\n",
    "        'min_samples_split': randint(2, 10),\n",
    "        'min_samples_leaf': randint(1, 5),\n",
    "        'max_features': ['sqrt', 'log2']\n",
    "    },\n",
    "    'KNeighborsClassifier': {\n",
    "        'n_neighbors': randint(3, 30),\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'algorithm': ['ball_tree', 'kd_tree', 'brute'],\n",
    "        'leaf_size': randint(10, 100)\n",
    "    },\n",
    "    'GaussianNB': {\n",
    "        'var_smoothing': uniform(1e-09, 1e-07)\n",
    "    },\n",
    "    'MLPClassifier': {\n",
    "        'hidden_layer_sizes': [(50, 50), (100,), (100, 50)],\n",
    "        'activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "        'solver': ['lbfgs', 'sgd', 'adam'],\n",
    "        'alpha': uniform(0.0001, 0.01),\n",
    "        'max_iter': randint(100, 1000)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Create a list of models to train (as example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    RandomForestClassifier(),\n",
    "    GradientBoostingClassifier(),\n",
    "    SVC(),\n",
    "    KNeighborsClassifier(),\n",
    "    GaussianNB()\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Loop over the models and fit  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model 1/5: RandomForestClassifier\n",
      "Best score: 0.400\n",
      "Best parameters: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 4, 'min_samples_split': 8, 'n_estimators': 130}\n",
      "\n",
      "Training Model 2/5: GradientBoostingClassifier\n",
      "Best score: 0.424\n",
      "Best parameters: {'learning_rate': 0.03703344575971452, 'max_depth': 8, 'max_features': 'log2', 'min_samples_leaf': 4, 'min_samples_split': 5, 'n_estimators': 68}\n",
      "\n",
      "Training Model 3/5: SVC\n",
      "Best score: 0.451\n",
      "Best parameters: {'C': 8.948624388292055, 'degree': 4, 'gamma': 0.30000000000000004, 'kernel': 'poly'}\n",
      "\n",
      "Training Model 4/5: KNeighborsClassifier\n",
      "Best score: 0.360\n",
      "Best parameters: {'algorithm': 'kd_tree', 'leaf_size': 31, 'n_neighbors': 9, 'weights': 'uniform'}\n",
      "\n",
      "Training Model 5/5: GaussianNB\n",
      "Best score: 0.384\n",
      "Best parameters: {'var_smoothing': 8.281483955122005e-08}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, model in enumerate(models):\n",
    "    print(f'Training Model {i+1}/{len(models)}: {str(model)[:-2]}')\n",
    "    # Define randomized grid search\n",
    "    random_search = RandomizedSearchCV(model, param_distributions[str(model)[:-2]], n_iter=10,cv=5, n_jobs=-1) # n_jobs means number of jobs to run in parallel. None means 1,\n",
    "                                                                                                                # -1 means using all processors 😈.\n",
    "    # Fit the randomized grid search to the data\n",
    "    random_search.fit(X_train, Y_train)\n",
    "    print(f'Best score: {random_search.best_score_:.3f}')\n",
    "    print(f'Best parameters: {random_search.best_params_}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
