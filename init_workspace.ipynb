{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initial code for our Hand Gesture Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML models imports\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# destribution models imports\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Data manipulation imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm as tqdm\n",
    "\n",
    "# visualisation models imports\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# image processing imports\n",
    "import skimage.io as io\n",
    "import cv2\n",
    "from skimage.transform import resize\n",
    "\n",
    "# dealing with files\n",
    "import os\n",
    "\n",
    "# visual dataset (to test randomized gridsearch not needed for now)\n",
    "from sklearn.datasets import make_hastie_10_2  # to test our models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Input:\n",
    "    images: a list or array-like object containing the input data, where each row represents an image\n",
    "    labels: a list or array-like object containing the corresponding output labels for each image\n",
    "\n",
    "Output:\n",
    "    x: a pandas DataFrame containing the input data (images)\n",
    "    y: a pandas Series containing the output (predicted) data (labels)\n",
    "\n",
    "Functionality:\n",
    "    This function takes two inputs, images and labels, and returns a tuple of input and output data,\n",
    "    respectively. First, it converts the input data (images) into a pandas DataFrame,\n",
    "    and then it adds a new column 'Target' to the DataFrame with corresponding output labels.\n",
    "    The input data is then extracted from the DataFrame by selecting all the columns except the last column,\n",
    "    which represents the output data. The output data is also extracted by selecting only the last column of the DataFrame.\n",
    "    Finally, the input and output data are returned as x and y, respectively. this function is used for dealing with images \n",
    "    easily and smoothly.\n",
    "Example:\n",
    "    x, y = prepareData(images,labels) \n",
    "\n",
    "'''\n",
    "def prepareData(images,labels):\n",
    "    df=pd.DataFrame(images) \n",
    "    df['Target'] = labels\n",
    "    x = df.iloc[:,:-1]  \n",
    "    y = df.iloc[:,-1] \n",
    "    return np.asarray(x),np.asarray(y)\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Input:\n",
    "    Gender: a list containing  \n",
    "\n",
    "Output:\n",
    "    Features: a list containing extracted features from the images\n",
    "    labels: a list containing extracted labels\n",
    "\n",
    "Functionality:\n",
    "    This function is used to process data from their directory and extract features from the images.\n",
    "\n",
    "Example:\n",
    "    Features, labels = LoadData() \n",
    "\n",
    "Note: \n",
    "    [*] Structure of Dataset foulder should be as follows in the project directory:\n",
    "\n",
    "                       Dataset\n",
    "                    men       Women\n",
    "              0 1 2 3 4 5    0 1 2 3 4 5\n",
    "           \n",
    "        -> each number is a directory contains the images of that class.\n",
    "\n",
    "    [*] this structure existed initially in the given dataset i didn't came up with :) \n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "def LoadData():\n",
    "    Features=[]\n",
    "    labels=[]\n",
    "\n",
    "    for gender in [\"men\",\"Women\"]:\n",
    "        datadir = r\"Dataset\\{}\".format(gender)\n",
    "        # loop over gender\n",
    "        for hand in os.listdir(datadir): \n",
    "            # loop over each class [0,1,2,3,4,5]\n",
    "            for img in os.listdir(datadir+ \"/\" +str(hand)):\n",
    "                # ignoring anything except images\n",
    "                if((img.split('.')[-1]).lower() not in ['jpg','png','jpeg']):\n",
    "                    continue\n",
    "\n",
    "                # loading our images\n",
    "                img_array=io.imread(datadir + \"/\" + str(hand) + \"/\" + img ,as_gray=True)  # approx 2500 * 4000\n",
    "\n",
    "                # append extracted features to Featurees list           \n",
    "                Features.append(FeatureExtraction(img_array)) \n",
    "\n",
    "                # append class of image.\n",
    "                labels.append(hand)     \n",
    "\n",
    "    return np.asarray(Features),np.asarray(labels)\n",
    "\n",
    "\n",
    "'''\n",
    "input:\n",
    "    images: an image of hand.\n",
    "\n",
    "output:\n",
    "    Extracted features ready to be used in training.\n",
    "\n",
    "Functionality: \n",
    "    This function is used for Extracting features from images.\n",
    "Example:\n",
    "    features = extractFeatures(image)\n",
    "'''\n",
    "def FeatureExtraction(image):\n",
    "    # this written code is an initial code for extracting features\n",
    "    resized = resize(image,(500,500))   # downscaing from approx 2500x4000 to 500x500\n",
    "\n",
    "    '''\n",
    "\n",
    "        TODO: Feature Extraction code should be implemented here.  \n",
    "\n",
    "    '''\n",
    "\n",
    "    image = np.array(resized).flatten() # flatten our image to be used as input vector to our model\n",
    "\n",
    "    return image\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.        , 1.        , 1.        , ..., 0.57579441, 0.57231646,\n",
       "         0.57919813],\n",
       "        [0.55088159, 0.56361918, 0.57627312, ..., 0.52987367, 0.53041833,\n",
       "         0.5319493 ],\n",
       "        [0.55018642, 0.54455157, 0.53863661, ..., 0.44312991, 0.43867312,\n",
       "         0.43528299],\n",
       "        ...,\n",
       "        [0.70066962, 0.69860664, 0.69564254, ..., 0.74752134, 0.74584222,\n",
       "         0.74773582],\n",
       "        [0.86934983, 0.87384208, 0.86957133, ..., 0.64137493, 0.64072342,\n",
       "         0.64580521],\n",
       "        [0.86898109, 0.86972548, 0.87227469, ..., 0.85155641, 0.85046445,\n",
       "         0.851305  ]]),\n",
       " array(['0', '0', '0', '0', '0', '1', '1', '1', '1', '1', '2', '2', '2',\n",
       "        '2', '2', '3', '3', '3', '3', '3', '4', '4', '4', '4', '5', '5',\n",
       "        '5', '5', '5', '0', '0', '0', '0', '0', '1', '1', '1', '1', '1',\n",
       "        '2', '2', '2', '2', '2', '3', '3', '3', '3', '3', '4', '4', '4',\n",
       "        '4', '4', '5', '5', '5', '5', '5'], dtype='<U1'))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LoadData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install imageio[pyav] (in case of having error while loading some of images try this one)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* # Selecting the best model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Define hyperparameter grids for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distributions = {\n",
    "    'RandomForestClassifier': {\n",
    "        'n_estimators': randint(50, 500),\n",
    "        'max_depth': randint(2, 20),\n",
    "        'min_samples_split': randint(2, 10),\n",
    "        'min_samples_leaf': randint(1, 5),\n",
    "        'max_features': ['sqrt', 'log2']\n",
    "    },\n",
    "    'GradientBoostingClassifier': {\n",
    "        'learning_rate': uniform(0.01, 0.2),\n",
    "        'n_estimators': randint(50, 500),\n",
    "        'max_depth': randint(2, 20),\n",
    "        'min_samples_split': randint(2, 10),\n",
    "        'min_samples_leaf': randint(1, 5),\n",
    "        'max_features': ['sqrt', 'log2']\n",
    "    },\n",
    "    'SVC': {\n",
    "        'C': uniform(0.01, 10),\n",
    "        'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "        'degree': randint(2, 5),\n",
    "        'gamma': ['scale', 'auto'] + list(np.arange(0.1, 1, 0.1))\n",
    "    },\n",
    "    'LogisticRegression': {\n",
    "        'C': uniform(0.01, 10),\n",
    "        'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "        'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "        'max_iter': randint(50, 500)\n",
    "    },\n",
    "    'DecisionTreeClassifier': {\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "        'splitter': ['best', 'random'],\n",
    "        'max_depth': randint(2, 20),\n",
    "        'min_samples_split': randint(2, 10),\n",
    "        'min_samples_leaf': randint(1, 5),\n",
    "        'max_features': ['sqrt', 'log2']\n",
    "    },\n",
    "    'KNeighborsClassifier': {\n",
    "        'n_neighbors': randint(3, 30),\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'algorithm': ['ball_tree', 'kd_tree', 'brute'],\n",
    "        'leaf_size': randint(10, 100)\n",
    "    },\n",
    "    'GaussianNB': {\n",
    "        'var_smoothing': uniform(1e-09, 1e-07)\n",
    "    },\n",
    "    'MLPClassifier': {\n",
    "        'hidden_layer_sizes': [(50, 50), (100,), (100, 50)],\n",
    "        'activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "        'solver': ['lbfgs', 'sgd', 'adam'],\n",
    "        'alpha': uniform(0.0001, 0.01),\n",
    "        'max_iter': randint(100, 1000)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Create a list of models to train (as example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    RandomForestClassifier(),\n",
    "    GradientBoostingClassifier(),\n",
    "    SVC(),\n",
    "    LogisticRegression(),\n",
    "    DecisionTreeClassifier(),\n",
    "    KNeighborsClassifier(),\n",
    "    GaussianNB(),\n",
    "    MLPClassifier()\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Loop over the models and fit  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load our dummy data to test the randomizedSearche function\n",
    "x,y = make_hastie_10_2()\n",
    "df = pd.DataFrame(x)\n",
    "df['Y'] = y\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2) # this function shuffles the data points, and splits the data into\n",
    "                                                  # 80% training set and 20% test set (indicated by test_size=0.2)\n",
    "X_train, Y_train = train.iloc[:, :-1], train.iloc[:, -1]\n",
    "X_test, Y_test = test.iloc[:, :-1], test.iloc[:, -1]\n",
    "# Fit a simple decision tree first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model 1/8: RandomForestClassifier\n",
      "Best score: 0.882\n",
      "Best parameters: {'max_depth': 18, 'max_features': 'sqrt', 'min_samples_leaf': 4, 'min_samples_split': 7, 'n_estimators': 302}\n",
      "\n",
      "Training Model 2/8: GradientBoostingClassifier\n",
      "Best score: 0.941\n",
      "Best parameters: {'learning_rate': 0.18591736381178015, 'max_depth': 5, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 251}\n",
      "\n",
      "Training Model 3/8: SVC\n",
      "Best score: 0.994\n",
      "Best parameters: {'C': 8.004579877520495, 'degree': 2, 'gamma': 0.7000000000000001, 'kernel': 'poly'}\n",
      "\n",
      "Training Model 4/8: LogisticRegression\n",
      "Best score: 0.481\n",
      "Best parameters: {'C': 1.1575614261989053, 'max_iter': 227, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "\n",
      "Training Model 5/8: DecisionTreeClassifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Khaled Hesham\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "20 fits failed out of a total of 50.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Khaled Hesham\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Khaled Hesham\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Khaled Hesham\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Khaled Hesham\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Khaled Hesham\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1291, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, prefer=prefer)(\n",
      "  File \"c:\\Users\\Khaled Hesham\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py\", line 1085, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"c:\\Users\\Khaled Hesham\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py\", line 901, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"c:\\Users\\Khaled Hesham\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py\", line 819, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"c:\\Users\\Khaled Hesham\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"c:\\Users\\Khaled Hesham\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 597, in __init__\n",
      "    self.results = batch()\n",
      "  File \"c:\\Users\\Khaled Hesham\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py\", line 288, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"c:\\Users\\Khaled Hesham\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py\", line 288, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"c:\\Users\\Khaled Hesham\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 117, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"c:\\Users\\Khaled Hesham\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 521, in _logistic_regression_path\n",
      "    alpha = (1.0 / C) * (1 - l1_ratio)\n",
      "TypeError: unsupported operand type(s) for -: 'int' and 'NoneType'\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Khaled Hesham\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Khaled Hesham\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Khaled Hesham\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Khaled Hesham\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Khaled Hesham\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Khaled Hesham\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 71, in _check_solver\n",
      "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Khaled Hesham\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [0.48135417        nan 0.48135417 0.48135417 0.48135417 0.4809375\n",
      " 0.48135417        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.762\n",
      "Best parameters: {'criterion': 'entropy', 'max_depth': 16, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 5, 'splitter': 'best'}\n",
      "\n",
      "Training Model 6/8: KNeighborsClassifier\n",
      "Best score: 0.712\n",
      "Best parameters: {'algorithm': 'ball_tree', 'leaf_size': 13, 'n_neighbors': 5, 'weights': 'distance'}\n",
      "\n",
      "Training Model 7/8: GaussianNB\n",
      "Best score: 0.973\n",
      "Best parameters: {'var_smoothing': 6.553132429912167e-08}\n",
      "\n",
      "Training Model 8/8: MLPClassifier\n",
      "Best score: 0.957\n",
      "Best parameters: {'activation': 'relu', 'alpha': 0.008916689358333677, 'hidden_layer_sizes': (100,), 'max_iter': 503, 'solver': 'sgd'}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Khaled Hesham\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (503) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "for i, model in enumerate(models):\n",
    "    print(f'Training Model {i+1}/{len(models)}: {str(model)[:-2]}')\n",
    "    # Define randomized grid search\n",
    "    random_search = RandomizedSearchCV(model, param_distributions[str(model)[:-2]], n_iter=10,cv=5, n_jobs=-1) # n_jobs means number of jobs to run in parallel. None means 1,\n",
    "                                                                                                                # -1 means using all processors ðŸ˜ˆ.\n",
    "    # Fit the randomized grid search to the data\n",
    "    random_search.fit(X_train, Y_train)\n",
    "    print(f'Best score: {random_search.best_score_:.3f}')\n",
    "    print(f'Best parameters: {random_search.best_params_}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
