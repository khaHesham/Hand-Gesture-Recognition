{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initial code for our Hand Gesture Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML models imports\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.feature import hog\n",
    "\n",
    "# destribution models imports\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Data manipulation imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm as tqdm\n",
    "\n",
    "# visualisation models imports\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# image processing imports\n",
    "import skimage.io as io\n",
    "import cv2\n",
    "from skimage.transform import resize\n",
    "\n",
    "# dealing with files\n",
    "import os\n",
    "\n",
    "# visual dataset (to test randomized gridsearch not needed for now)\n",
    "from sklearn.datasets import make_hastie_10_2  # to test our models\n",
    "\n",
    "# from utils import prepareData, LoadData, FeatureExtraction, preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_hsv=np.array([0,40,80],dtype='uint8')\n",
    "upper_hsv=np.array([0,255,255],dtype='uint8')\n",
    "def skin_detector_hsv(bgr_image):\n",
    "    hsv_image=cv2.cvtColor(bgr_image,cv2.COLOR_BGR2HSV)\n",
    "    skin_region=cv2.inRange(hsv_image,lower_hsv,upper_hsv)\n",
    "    return skin_region\n",
    "\n",
    "lower_hsv_2=np.array([0,50,0],dtype='uint8')\n",
    "upper_hsv_2=np.array([120,150,255],dtype='uint8')\n",
    "def skin_detector_hsv_2(bgr_image):\n",
    "    hsv_image=cv2.cvtColor(bgr_image,cv2.COLOR_BGR2HSV)\n",
    "    skin_region=cv2.inRange(hsv_image,lower_hsv_2,upper_hsv_2)\n",
    "    return skin_region\n",
    "\n",
    "lower_ycrcb=np.array([0,133,77], dtype='uint8')\n",
    "upper_ycrcb=np.array([255,173,127],dtype='uint8')\n",
    "def skin_detector_ycrcb(bgr_image):\n",
    "    hsv_image=cv2.cvtColor(bgr_image,cv2.COLOR_BGR2HSV)\n",
    "    skin_region=cv2.inRange(hsv_image,lower_ycrcb,upper_ycrcb)\n",
    "    return skin_region\n",
    "\n",
    "\n",
    "def bgr_skin(b,g,r):\n",
    "    e1= bool( (r>95) and (g>40) and (b>20) and ((max(r,max(b,g))- min(r,min(g,b))) > 15) and ( abs(int(r)-int(g)) >15) and (r>g) and(r>b) )\n",
    "    e2=bool( (r>220) and (g>210) and (b>170) and (abs(int(r)- int(g)) <=15) and (r>b) and (g>b) )\n",
    "    return e1 or e2\n",
    "\n",
    "def skin_detector_bgr(bgr_image):\n",
    "\n",
    "    h=bgr_image.shape[0]\n",
    "    w=bgr_image.shape[1]\n",
    "    res=np.zeros((h,w,1),dtype='uint8')\n",
    "    for y in range(0,h):\n",
    "        for x in range(0,w):\n",
    "            (b,g,r)=bgr_image[y,x]\n",
    "            if(bgr_skin(b,g,r)):\n",
    "                res[y,x]=255    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment(image):\n",
    "    blured_image = cv2.GaussianBlur(image, (7, 7), 0)\n",
    "    ycbcr_image = cv2.cvtColor(blured_image, cv2.COLOR_BGR2YCrCb)\n",
    "    # Extract the Cr channel\n",
    "    cr_channel = ycbcr_image[:,:,1]\n",
    "\n",
    "    # Apply thresholding to obtain a binary image\n",
    "    _, binary_img = cv2.threshold(cr_channel,0,255,cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "\n",
    "    # Define the structuring element for the closing operation\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (10, 10))\n",
    "\n",
    "    # Perform the closing operation\n",
    "    closed_img = cv2.morphologyEx(binary_img, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "    # Find the contours in the binary image\n",
    "    contours, hierarchy = cv2.findContours(closed_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Filling the contours on a copy of the original image\n",
    "    # img_contours = cv2.cvtColor(cr_channel, cv2.COLOR_GRAY2BGR)\n",
    "    # cv2.drawContours(img_contours, contours, -1, (0, 0, 0), -1)\n",
    "\n",
    "    segmented_image = closed_img.copy()\n",
    "    cv2.drawContours(segmented_image, contours, -1, 255, -1)\n",
    "\n",
    "    return segmented_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FeatureExtraction(image):\n",
    "        \n",
    "    # Extract the hog features\n",
    "    # block_norm uses L2 norm with hysterisis for reducing effect of illuminacity\n",
    "    # transform_sqrt for applying gamma correction\n",
    "    preprocessed_image = segment(image)\n",
    "    \n",
    "    resized_image = resize(preprocessed_image,(128,256))   # downscaing from approx 2500x4000 to 500x500\n",
    "\n",
    "    hog_features = hog(resized_image, block_norm='L2-Hys', feature_vector=True, transform_sqrt=True, pixels_per_cell=(9, 9), cells_per_block=(2, 2))\n",
    "\n",
    "    return hog_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadData():\n",
    "    Features=[]\n",
    "    labels=[]\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    for gender in [\"men\",\"Women\"]:\n",
    "        datadir = r\"Dataset\\{}\".format(gender)\n",
    "        # loop over gender\n",
    "        for hand in os.listdir(datadir): \n",
    "            # loop over each class [0,1,2,3,4,5]\n",
    "            for img in os.listdir(datadir+ \"/\" +str(hand)):\n",
    "                # ignoring anything except images\n",
    "                if((img.split('.')[-1]).lower() not in ['jpg','png','jpeg']):\n",
    "                    continue\n",
    "\n",
    "                # loading our images\n",
    "                img_array=io.imread(datadir + \"/\" + str(hand) + \"/\" + img )  # approx 2500 * 4000\n",
    "\n",
    "                # append extracted features to Featurees list           \n",
    "                Features.append(FeatureExtraction(img_array)) \n",
    "\n",
    "                # append class of image.\n",
    "                labels.append(hand)  \n",
    "\n",
    "                print(f'image Number: {i}')\n",
    "                i+=1\n",
    "\n",
    "    return np.asarray(Features),np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image Number: 0\n",
      "image Number: 1\n",
      "image Number: 2\n",
      "image Number: 3\n",
      "image Number: 4\n",
      "image Number: 5\n",
      "image Number: 6\n",
      "image Number: 7\n",
      "image Number: 8\n",
      "image Number: 9\n",
      "image Number: 10\n",
      "image Number: 11\n",
      "image Number: 12\n",
      "image Number: 13\n",
      "image Number: 14\n",
      "image Number: 15\n",
      "image Number: 16\n",
      "image Number: 17\n",
      "image Number: 18\n",
      "image Number: 19\n",
      "image Number: 20\n",
      "image Number: 21\n",
      "image Number: 22\n",
      "image Number: 23\n",
      "image Number: 24\n",
      "image Number: 25\n",
      "image Number: 26\n",
      "image Number: 27\n",
      "image Number: 28\n",
      "image Number: 29\n",
      "image Number: 30\n",
      "image Number: 31\n",
      "image Number: 32\n",
      "image Number: 33\n",
      "image Number: 34\n",
      "image Number: 35\n",
      "image Number: 36\n",
      "image Number: 37\n",
      "image Number: 38\n",
      "image Number: 39\n",
      "image Number: 40\n",
      "image Number: 41\n",
      "image Number: 42\n",
      "image Number: 43\n",
      "image Number: 44\n",
      "image Number: 45\n",
      "image Number: 46\n",
      "image Number: 47\n",
      "image Number: 48\n",
      "image Number: 49\n",
      "image Number: 50\n",
      "image Number: 51\n",
      "image Number: 52\n",
      "image Number: 53\n",
      "image Number: 54\n",
      "image Number: 55\n",
      "image Number: 56\n",
      "image Number: 57\n",
      "image Number: 58\n",
      "image Number: 59\n"
     ]
    }
   ],
   "source": [
    "Features, labels = LoadData() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('labels.npy', labels)\n",
    "np.save('features.npy', Features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### to load our extracted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0' '0' '0' '0' '0' '1' '1' '1' '1' '1' '2' '2' '2' '2' '2' '3' '3' '3'\n",
      " '3' '3' '4' '4' '4' '4' '4' '5' '5' '5' '5' '5' '0' '0' '0' '0' '0' '1'\n",
      " '1' '1' '1' '1' '2' '2' '2' '2' '2' '3' '3' '3' '3' '3' '4' '4' '4' '4'\n",
      " '4' '5' '5' '5' '5' '5']\n"
     ]
    }
   ],
   "source": [
    "# test = np.load('labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0', '0', '0', '0', '0', '1', '1', '1', '1', '2', '2', '2', '2',\n",
       "       '2', '3', '3', '3', '3', '3', '4', '4', '4', '4', '5', '5', '5',\n",
       "       '5', '5', '0', '0', '0', '0', '0', '1', '1', '1', '1', '1', '2',\n",
       "       '2', '2', '2', '2', '3', '3', '3', '3', '3', '4', '4', '4', '4',\n",
       "       '4', '5', '5', '5', '5', '5'], dtype='<U1')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#! pip install imageio[pyav] (in case of having error while loading some of images try this one)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* # Selecting the best model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Define hyperparameter grids for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distributions = {\n",
    "    'RandomForestClassifier': {\n",
    "        'n_estimators': randint(50, 500),\n",
    "        'max_depth': randint(2, 20),\n",
    "        'min_samples_split': randint(2, 10),\n",
    "        'min_samples_leaf': randint(1, 5),\n",
    "        'max_features': ['sqrt', 'log2']\n",
    "    },\n",
    "    'GradientBoostingClassifier': {\n",
    "        'learning_rate': uniform(0.01, 0.2),\n",
    "        'n_estimators': randint(50, 500),\n",
    "        'max_depth': randint(2, 20),\n",
    "        'min_samples_split': randint(2, 10),\n",
    "        'min_samples_leaf': randint(1, 5),\n",
    "        'max_features': ['sqrt', 'log2']\n",
    "    },\n",
    "    'SVC': {\n",
    "        'C': uniform(0.01, 10),\n",
    "        'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "        'degree': randint(2, 5),\n",
    "        'gamma': ['scale', 'auto'] + list(np.arange(0.1, 1, 0.1))\n",
    "    },\n",
    "    'LogisticRegression': {\n",
    "        'C': uniform(0.01, 10),\n",
    "        'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "        'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "        'max_iter': randint(50, 500)\n",
    "    },\n",
    "    'DecisionTreeClassifier': {\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "        'splitter': ['best', 'random'],\n",
    "        'max_depth': randint(2, 20),\n",
    "        'min_samples_split': randint(2, 10),\n",
    "        'min_samples_leaf': randint(1, 5),\n",
    "        'max_features': ['sqrt', 'log2']\n",
    "    },\n",
    "    'KNeighborsClassifier': {\n",
    "        'n_neighbors': randint(3, 30),\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'algorithm': ['ball_tree', 'kd_tree', 'brute'],\n",
    "        'leaf_size': randint(10, 100)\n",
    "    },\n",
    "    'GaussianNB': {\n",
    "        'var_smoothing': uniform(1e-09, 1e-07)\n",
    "    },\n",
    "    'MLPClassifier': {\n",
    "        'hidden_layer_sizes': [(50, 50), (100,), (100, 50)],\n",
    "        'activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "        'solver': ['lbfgs', 'sgd', 'adam'],\n",
    "        'alpha': uniform(0.0001, 0.01),\n",
    "        'max_iter': randint(100, 1000)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Create a list of models to train (as example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    RandomForestClassifier(),\n",
    "    SVC(),\n",
    "    GaussianNB()\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Loop over the models and fit  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load our dummy data to test the randomizedSearche function\n",
    "x,y = make_hastie_10_2()\n",
    "df = pd.DataFrame(x)\n",
    "df['Y'] = y\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2) # this function shuffles the data points, and splits the data into\n",
    "                                                  # 80% training set and 20% test set (indicated by test_size=0.2)\n",
    "X_train, Y_train = train.iloc[:, :-1], train.iloc[:, -1]\n",
    "X_test, Y_test = test.iloc[:, :-1], test.iloc[:, -1]\n",
    "# Fit a simple decision tree first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model 1/3: RandomForestClassifier\n",
      "Best score: 0.881\n",
      "Best parameters: {'max_depth': 19, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 4, 'n_estimators': 179}\n",
      "\n",
      "Training Model 2/3: SVC\n",
      "Best score: 0.963\n",
      "Best parameters: {'C': 5.8817763888073795, 'degree': 4, 'gamma': 0.4, 'kernel': 'poly'}\n",
      "\n",
      "Training Model 3/3: GaussianNB\n",
      "Best score: 0.979\n",
      "Best parameters: {'var_smoothing': 1.066596175409692e-08}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, model in enumerate(models):\n",
    "    print(f'Training Model {i+1}/{len(models)}: {str(model)[:-2]}')\n",
    "    # Define randomized grid search\n",
    "    random_search = RandomizedSearchCV(model, param_distributions[str(model)[:-2]], n_iter=10,cv=5, n_jobs=-1) # n_jobs means number of jobs to run in parallel. None means 1,\n",
    "                                                                                                                # -1 means using all processors ðŸ˜ˆ.\n",
    "    # Fit the randomized grid search to the data\n",
    "    random_search.fit(X_train, Y_train)\n",
    "    print(f'Best score: {random_search.best_score_:.3f}')\n",
    "    print(f'Best parameters: {random_search.best_params_}\\n')\n",
    "\n",
    "    with open(\"analysis.txt\", \"a\") as file:\n",
    "        file.write(f'Training Model {i+1}/{len(models)}: {str(model)[:-2]})\n",
    "        print(f'Best score: {random_search.best_score_:.3f}')\n",
    "        print(f'Best parameters: {random_search.best_params_}\\n')\n",
    "        file.write(\"\\n\\n\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
