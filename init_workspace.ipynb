{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initial code for our Hand Gesture Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML models imports\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.feature import hog\n",
    "\n",
    "# destribution models imports\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Data manipulation imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm as tqdm\n",
    "\n",
    "# visualisation models imports\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# image processing imports\n",
    "import skimage.io as io\n",
    "import cv2\n",
    "from skimage.transform import resize\n",
    "\n",
    "# dealing with files\n",
    "import os\n",
    "\n",
    "# visual dataset (to test randomized gridsearch not needed for now)\n",
    "from sklearn.datasets import make_hastie_10_2  # to test our models\n",
    "\n",
    "# from utils import prepareData, LoadData, FeatureExtraction, preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment(image):\n",
    "    blured_image = cv2.GaussianBlur(image, (7, 7), 0)\n",
    "    ycbcr_image = cv2.cvtColor(blured_image, cv2.COLOR_BGR2YCrCb)\n",
    "    # Extract the Cr channel\n",
    "    cr_channel = ycbcr_image[:,:,1]\n",
    "\n",
    "    # Apply thresholding to obtain a binary image\n",
    "    _, binary_img = cv2.threshold(cr_channel,0,255,cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "\n",
    "    # Define the structuring element for the closing operation\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (25, 25))\n",
    "\n",
    "    # Perform the closing operation\n",
    "    closed_img = cv2.morphologyEx(binary_img, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "    # Find the contours in the binary image\n",
    "    contours, hierarchy = cv2.findContours(closed_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Filling the contours on a copy of the original image\n",
    "    img_contours = cv2.cvtColor(cr_channel, cv2.COLOR_GRAY2BGR)\n",
    "    cv2.drawContours(img_contours, contours, -1, (0, 0, 0), -1)\n",
    "\n",
    "    return img_contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FeatureExtraction(image):\n",
    "    # this written code is an initial code for extracting features\n",
    "\n",
    "    '''\n",
    "\n",
    "        TODO: Feature Extraction code should be implemented here.  \n",
    "\n",
    "    '''\n",
    "    \n",
    "    resized_image = resize(image,(1000,1000))   # downscaing from approx 2500x4000 to 500x500\n",
    "    \n",
    "    # Extract the hog features\n",
    "    # block_norm uses L2 norm with hysterisis for reducing effect of illuminacity\n",
    "    # transform_sqrt for applying gamma correction\n",
    "    segmented = segment(image)\n",
    "\n",
    "    hog_features = hog(segmented, block_norm='L2-Hys', feature_vector=True, transform_sqrt=True, channel_axis=2)\n",
    "\n",
    "    # image = np.array(resized).flatten() # flatten our image to be used as input vector to our model\n",
    "\n",
    "    return hog_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadData():\n",
    "    Features=[]\n",
    "    labels=[]\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    for gender in [\"men\",\"Women\"]:\n",
    "        datadir = r\"Dataset\\{}\".format(gender)\n",
    "        # loop over gender\n",
    "        for hand in os.listdir(datadir): \n",
    "            # loop over each class [0,1,2,3,4,5]\n",
    "            for img in os.listdir(datadir+ \"/\" +str(hand)):\n",
    "                # ignoring anything except images\n",
    "                if((img.split('.')[-1]).lower() not in ['jpg','png','jpeg']):\n",
    "                    continue\n",
    "\n",
    "                # loading our images\n",
    "                img_array=io.imread(datadir + \"/\" + str(hand) + \"/\" + img )  # approx 2500 * 4000\n",
    "\n",
    "                # append extracted features to Featurees list           \n",
    "                Features.append(FeatureExtraction(img_array)) \n",
    "\n",
    "                # append class of image.\n",
    "                labels.append(hand)  \n",
    "\n",
    "                print(f'image Number: {i}')\n",
    "                i+=1\n",
    "\n",
    "    return np.asarray(Features),np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image Number: 0\n",
      "image Number: 1\n",
      "image Number: 2\n",
      "image Number: 3\n",
      "image Number: 4\n",
      "image Number: 5\n",
      "image Number: 6\n",
      "image Number: 7\n",
      "image Number: 8\n",
      "image Number: 9\n",
      "image Number: 10\n",
      "image Number: 11\n",
      "image Number: 12\n",
      "image Number: 13\n",
      "image Number: 14\n",
      "image Number: 15\n",
      "image Number: 16\n",
      "image Number: 17\n",
      "image Number: 18\n",
      "image Number: 19\n",
      "image Number: 20\n",
      "image Number: 21\n",
      "image Number: 22\n",
      "image Number: 23\n",
      "image Number: 24\n",
      "image Number: 25\n",
      "image Number: 26\n",
      "image Number: 27\n",
      "image Number: 28\n",
      "image Number: 29\n",
      "image Number: 30\n",
      "image Number: 31\n",
      "image Number: 32\n",
      "image Number: 33\n",
      "image Number: 34\n",
      "image Number: 35\n",
      "image Number: 36\n",
      "image Number: 37\n",
      "image Number: 38\n",
      "image Number: 39\n",
      "image Number: 40\n",
      "image Number: 41\n",
      "image Number: 42\n",
      "image Number: 43\n",
      "image Number: 44\n",
      "image Number: 45\n",
      "image Number: 46\n",
      "image Number: 47\n",
      "image Number: 48\n",
      "image Number: 49\n",
      "image Number: 50\n",
      "image Number: 51\n",
      "image Number: 52\n",
      "image Number: 53\n",
      "image Number: 54\n",
      "image Number: 55\n",
      "image Number: 56\n",
      "image Number: 57\n",
      "image Number: 58\n",
      "image Number: 59\n",
      "image Number: 60\n",
      "image Number: 61\n",
      "image Number: 62\n",
      "image Number: 63\n",
      "image Number: 64\n",
      "image Number: 65\n",
      "image Number: 66\n",
      "image Number: 67\n",
      "image Number: 68\n",
      "image Number: 69\n",
      "image Number: 70\n",
      "image Number: 71\n",
      "image Number: 72\n",
      "image Number: 73\n",
      "image Number: 74\n",
      "image Number: 75\n",
      "image Number: 76\n",
      "image Number: 77\n",
      "image Number: 78\n",
      "image Number: 79\n",
      "image Number: 80\n",
      "image Number: 81\n",
      "image Number: 82\n",
      "image Number: 83\n",
      "image Number: 84\n",
      "image Number: 85\n",
      "image Number: 86\n",
      "image Number: 87\n",
      "image Number: 88\n",
      "image Number: 89\n",
      "image Number: 90\n",
      "image Number: 91\n",
      "image Number: 92\n",
      "image Number: 93\n",
      "image Number: 94\n",
      "image Number: 95\n",
      "image Number: 96\n",
      "image Number: 97\n",
      "image Number: 98\n",
      "image Number: 99\n",
      "image Number: 100\n",
      "image Number: 101\n",
      "image Number: 102\n",
      "image Number: 103\n",
      "image Number: 104\n",
      "image Number: 105\n",
      "image Number: 106\n",
      "image Number: 107\n",
      "image Number: 108\n",
      "image Number: 109\n",
      "image Number: 110\n",
      "image Number: 111\n",
      "image Number: 112\n",
      "image Number: 113\n",
      "image Number: 114\n",
      "image Number: 115\n",
      "image Number: 116\n",
      "image Number: 117\n",
      "image Number: 118\n",
      "image Number: 119\n",
      "image Number: 120\n",
      "image Number: 121\n",
      "image Number: 122\n",
      "image Number: 123\n",
      "image Number: 124\n",
      "image Number: 125\n",
      "image Number: 126\n",
      "image Number: 127\n",
      "image Number: 128\n",
      "image Number: 129\n",
      "image Number: 130\n",
      "image Number: 131\n",
      "image Number: 132\n",
      "image Number: 133\n",
      "image Number: 134\n",
      "image Number: 135\n",
      "image Number: 136\n",
      "image Number: 137\n",
      "image Number: 138\n",
      "image Number: 139\n",
      "image Number: 140\n",
      "image Number: 141\n",
      "image Number: 142\n",
      "image Number: 143\n",
      "image Number: 144\n",
      "image Number: 145\n",
      "image Number: 146\n",
      "image Number: 147\n",
      "image Number: 148\n",
      "image Number: 149\n",
      "image Number: 150\n",
      "image Number: 151\n",
      "image Number: 152\n",
      "image Number: 153\n",
      "image Number: 154\n",
      "image Number: 155\n",
      "image Number: 156\n",
      "image Number: 157\n",
      "image Number: 158\n",
      "image Number: 159\n",
      "image Number: 160\n",
      "image Number: 161\n",
      "image Number: 162\n",
      "image Number: 163\n",
      "image Number: 164\n",
      "image Number: 165\n",
      "image Number: 166\n",
      "image Number: 167\n",
      "image Number: 168\n",
      "image Number: 169\n",
      "image Number: 170\n",
      "image Number: 171\n",
      "image Number: 172\n",
      "image Number: 173\n",
      "image Number: 174\n",
      "image Number: 175\n",
      "image Number: 176\n",
      "image Number: 177\n",
      "image Number: 178\n",
      "image Number: 179\n",
      "image Number: 180\n",
      "image Number: 181\n",
      "image Number: 182\n",
      "image Number: 183\n",
      "image Number: 184\n",
      "image Number: 185\n",
      "image Number: 186\n",
      "image Number: 187\n",
      "image Number: 188\n",
      "image Number: 189\n",
      "image Number: 190\n",
      "image Number: 191\n",
      "image Number: 192\n",
      "image Number: 193\n",
      "image Number: 194\n",
      "image Number: 195\n",
      "image Number: 196\n",
      "image Number: 197\n",
      "image Number: 198\n",
      "image Number: 199\n",
      "image Number: 200\n",
      "image Number: 201\n",
      "image Number: 202\n",
      "image Number: 203\n",
      "image Number: 204\n",
      "image Number: 205\n",
      "image Number: 206\n",
      "image Number: 207\n",
      "image Number: 208\n",
      "image Number: 209\n",
      "image Number: 210\n",
      "image Number: 211\n",
      "image Number: 212\n",
      "image Number: 213\n",
      "image Number: 214\n",
      "image Number: 215\n",
      "image Number: 216\n",
      "image Number: 217\n",
      "image Number: 218\n",
      "image Number: 219\n",
      "image Number: 220\n",
      "image Number: 221\n",
      "image Number: 222\n",
      "image Number: 223\n",
      "image Number: 224\n",
      "image Number: 225\n",
      "image Number: 226\n",
      "image Number: 227\n",
      "image Number: 228\n",
      "image Number: 229\n",
      "image Number: 230\n",
      "image Number: 231\n",
      "image Number: 232\n",
      "image Number: 233\n",
      "image Number: 234\n",
      "image Number: 235\n",
      "image Number: 236\n",
      "image Number: 237\n",
      "image Number: 238\n",
      "image Number: 239\n",
      "image Number: 240\n",
      "image Number: 241\n",
      "image Number: 242\n",
      "image Number: 243\n",
      "image Number: 244\n",
      "image Number: 245\n",
      "image Number: 246\n",
      "image Number: 247\n",
      "image Number: 248\n",
      "image Number: 249\n",
      "image Number: 250\n",
      "image Number: 251\n",
      "image Number: 252\n",
      "image Number: 253\n",
      "image Number: 254\n",
      "image Number: 255\n",
      "image Number: 256\n",
      "image Number: 257\n",
      "image Number: 258\n",
      "image Number: 259\n",
      "image Number: 260\n",
      "image Number: 261\n",
      "image Number: 262\n",
      "image Number: 263\n",
      "image Number: 264\n",
      "image Number: 265\n",
      "image Number: 266\n",
      "image Number: 267\n",
      "image Number: 268\n",
      "image Number: 269\n",
      "image Number: 270\n",
      "image Number: 271\n",
      "image Number: 272\n",
      "image Number: 273\n",
      "image Number: 274\n",
      "image Number: 275\n",
      "image Number: 276\n",
      "image Number: 277\n",
      "image Number: 278\n",
      "image Number: 279\n",
      "image Number: 280\n",
      "image Number: 281\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 91.1 MiB for an array with shape (2592, 4608) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m Features, labels \u001b[39m=\u001b[39m LoadData() \n",
      "Cell \u001b[1;32mIn[15], line 21\u001b[0m, in \u001b[0;36mLoadData\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m img_array\u001b[39m=\u001b[39mio\u001b[39m.\u001b[39mimread(datadir \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(hand) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m img )  \u001b[39m# approx 2500 * 4000\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39m# append extracted features to Featurees list           \u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m Features\u001b[39m.\u001b[39mappend(FeatureExtraction(img_array)) \n\u001b[0;32m     23\u001b[0m \u001b[39m# append class of image.\u001b[39;00m\n\u001b[0;32m     24\u001b[0m labels\u001b[39m.\u001b[39mappend(hand)  \n",
      "Cell \u001b[1;32mIn[12], line 17\u001b[0m, in \u001b[0;36mFeatureExtraction\u001b[1;34m(image)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39m# Extract the hog features\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[39m# block_norm uses L2 norm with hysterisis for reducing effect of illuminacity\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39m# transform_sqrt for applying gamma correction\u001b[39;00m\n\u001b[0;32m     15\u001b[0m segmented \u001b[39m=\u001b[39m segment(image)\n\u001b[1;32m---> 17\u001b[0m hog_features \u001b[39m=\u001b[39m hog(segmented, block_norm\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mL2-Hys\u001b[39;49m\u001b[39m'\u001b[39;49m, feature_vector\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, transform_sqrt\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, channel_axis\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[0;32m     19\u001b[0m \u001b[39m# image = np.array(resized).flatten() # flatten our image to be used as input vector to our model\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[39mreturn\u001b[39;00m hog_features\n",
      "File \u001b[1;32mc:\\Users\\Khaled Hesham\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\skimage\\_shared\\utils.py:359\u001b[0m, in \u001b[0;36mchannel_as_last_axis.__call__.<locals>.fixed_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    356\u001b[0m kwargs[\u001b[39m\"\u001b[39m\u001b[39mchannel_axis\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m    358\u001b[0m \u001b[39m# Call the function with the fixed arguments\u001b[39;00m\n\u001b[1;32m--> 359\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39mnew_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    360\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmultichannel_output:\n\u001b[0;32m    361\u001b[0m     out \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmoveaxis(out, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, channel_axis[\u001b[39m0\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\Khaled Hesham\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\skimage\\feature\\_hog.py:233\u001b[0m, in \u001b[0;36mhog\u001b[1;34m(image, orientations, pixels_per_cell, cells_per_block, block_norm, visualize, transform_sqrt, feature_vector, channel_axis)\u001b[0m\n\u001b[0;32m    230\u001b[0m g_row \u001b[39m=\u001b[39m g_row\u001b[39m.\u001b[39mastype(\u001b[39mfloat\u001b[39m, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    231\u001b[0m g_col \u001b[39m=\u001b[39m g_col\u001b[39m.\u001b[39mastype(\u001b[39mfloat\u001b[39m, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m--> 233\u001b[0m _hoghistogram\u001b[39m.\u001b[39;49mhog_histograms(g_col, g_row, c_col, c_row, s_col, s_row,\n\u001b[0;32m    234\u001b[0m                              n_cells_col, n_cells_row,\n\u001b[0;32m    235\u001b[0m                              orientations, orientation_histogram)\n\u001b[0;32m    237\u001b[0m \u001b[39m# now compute the histogram for each cell\u001b[39;00m\n\u001b[0;32m    238\u001b[0m hog_image \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32m_hoghistogram.pyx:116\u001b[0m, in \u001b[0;36mskimage.feature._hoghistogram.hog_histograms\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 91.1 MiB for an array with shape (2592, 4608) and data type float64"
     ]
    }
   ],
   "source": [
    "Features, labels = LoadData() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('labels.npy', labels)\n",
    "np.save('features.npy', Features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### to load our extracted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = np.load('labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0', '0', '0', '0', '0', '1', '1', '1', '1', '2', '2', '2', '2',\n",
       "       '2', '3', '3', '3', '3', '3', '4', '4', '4', '4', '5', '5', '5',\n",
       "       '5', '5', '0', '0', '0', '0', '0', '1', '1', '1', '1', '1', '2',\n",
       "       '2', '2', '2', '2', '3', '3', '3', '3', '3', '4', '4', '4', '4',\n",
       "       '4', '5', '5', '5', '5', '5'], dtype='<U1')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#! pip install imageio[pyav] (in case of having error while loading some of images try this one)\n",
    "labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* # Selecting the best model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Define hyperparameter grids for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distributions = {\n",
    "    'RandomForestClassifier': {\n",
    "        'n_estimators': randint(50, 500),\n",
    "        'max_depth': randint(2, 20),\n",
    "        'min_samples_split': randint(2, 10),\n",
    "        'min_samples_leaf': randint(1, 5),\n",
    "        'max_features': ['sqrt', 'log2']\n",
    "    },\n",
    "    'GradientBoostingClassifier': {\n",
    "        'learning_rate': uniform(0.01, 0.2),\n",
    "        'n_estimators': randint(50, 500),\n",
    "        'max_depth': randint(2, 20),\n",
    "        'min_samples_split': randint(2, 10),\n",
    "        'min_samples_leaf': randint(1, 5),\n",
    "        'max_features': ['sqrt', 'log2']\n",
    "    },\n",
    "    'SVC': {\n",
    "        'C': uniform(0.01, 10),\n",
    "        'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "        'degree': randint(2, 5),\n",
    "        'gamma': ['scale', 'auto'] + list(np.arange(0.1, 1, 0.1))\n",
    "    },\n",
    "    'LogisticRegression': {\n",
    "        'C': uniform(0.01, 10),\n",
    "        'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "        'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "        'max_iter': randint(50, 500)\n",
    "    },\n",
    "    'DecisionTreeClassifier': {\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "        'splitter': ['best', 'random'],\n",
    "        'max_depth': randint(2, 20),\n",
    "        'min_samples_split': randint(2, 10),\n",
    "        'min_samples_leaf': randint(1, 5),\n",
    "        'max_features': ['sqrt', 'log2']\n",
    "    },\n",
    "    'KNeighborsClassifier': {\n",
    "        'n_neighbors': randint(3, 30),\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'algorithm': ['ball_tree', 'kd_tree', 'brute'],\n",
    "        'leaf_size': randint(10, 100)\n",
    "    },\n",
    "    'GaussianNB': {\n",
    "        'var_smoothing': uniform(1e-09, 1e-07)\n",
    "    },\n",
    "    'MLPClassifier': {\n",
    "        'hidden_layer_sizes': [(50, 50), (100,), (100, 50)],\n",
    "        'activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "        'solver': ['lbfgs', 'sgd', 'adam'],\n",
    "        'alpha': uniform(0.0001, 0.01),\n",
    "        'max_iter': randint(100, 1000)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Create a list of models to train (as example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    RandomForestClassifier(),\n",
    "    SVC(),\n",
    "    GaussianNB()\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Loop over the models and fit  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load our dummy data to test the randomizedSearche function\n",
    "x,y = make_hastie_10_2()\n",
    "df = pd.DataFrame(x)\n",
    "df['Y'] = y\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2) # this function shuffles the data points, and splits the data into\n",
    "                                                  # 80% training set and 20% test set (indicated by test_size=0.2)\n",
    "X_train, Y_train = train.iloc[:, :-1], train.iloc[:, -1]\n",
    "X_test, Y_test = test.iloc[:, :-1], test.iloc[:, -1]\n",
    "# Fit a simple decision tree first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model 1/3: RandomForestClassifier\n",
      "Best score: 0.881\n",
      "Best parameters: {'max_depth': 19, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 4, 'n_estimators': 179}\n",
      "\n",
      "Training Model 2/3: SVC\n",
      "Best score: 0.963\n",
      "Best parameters: {'C': 5.8817763888073795, 'degree': 4, 'gamma': 0.4, 'kernel': 'poly'}\n",
      "\n",
      "Training Model 3/3: GaussianNB\n",
      "Best score: 0.979\n",
      "Best parameters: {'var_smoothing': 1.066596175409692e-08}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, model in enumerate(models):\n",
    "    print(f'Training Model {i+1}/{len(models)}: {str(model)[:-2]}')\n",
    "    # Define randomized grid search\n",
    "    random_search = RandomizedSearchCV(model, param_distributions[str(model)[:-2]], n_iter=10,cv=5, n_jobs=-1) # n_jobs means number of jobs to run in parallel. None means 1,\n",
    "                                                                                                                # -1 means using all processors ðŸ˜ˆ.\n",
    "    # Fit the randomized grid search to the data\n",
    "    random_search.fit(X_train, Y_train)\n",
    "    print(f'Best score: {random_search.best_score_:.3f}')\n",
    "    print(f'Best parameters: {random_search.best_params_}\\n')\n",
    "\n",
    "    with open(\"analysis.txt\", \"a\") as file:\n",
    "        file.write(f'Training Model {i+1}/{len(models)}: {str(model)[:-2]})\n",
    "        print(f'Best score: {random_search.best_score_:.3f}')\n",
    "        print(f'Best parameters: {random_search.best_params_}\\n')\n",
    "        file.write(\"\\n\\n\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
